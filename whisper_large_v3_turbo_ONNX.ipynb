{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwcQ-E3j0dLx"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------\n",
        "# 1. System packages (run once)\n",
        "# -------------------------------------------------\n",
        "!apt-get -qq update && apt-get -qq install -y ffmpeg   # update apt index (quiet) and install ffmpeg (required for audio I/O)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. Install Python dependencies (run once)\n",
        "# -------------------------------------------------\n",
        "# install/upgrade the required Python libs (quiet)\n",
        "# ðŸ¤— Transformers â€“ model & generation utilities\n",
        "# ðŸ¤— Datasets â€“ optional, not used directly here\n",
        "# ðŸ¤— Accelerate â€“ optional, for multiâ€‘GPU handling\n",
        "# ðŸ¤— Hub client â€“ needed for snapshot_download\n",
        "# Optimum + ONNX Runtime integration\n",
        "# audio I/O, resampling, progress bars, etc.\n",
        "!pip install -q --upgrade \\\n",
        "    \"transformers\" \\\n",
        "    \"datasets\" \\\n",
        "    \"accelerate\" \\\n",
        "    \"huggingface_hub\" \\\n",
        "    \"optimum[onnxruntime]\" \\\n",
        "    \"soundfile\" \\\n",
        "    \"librosa\" \\\n",
        "    \"ffmpeg-python\" \\\n",
        "    \"tqdm\"\n",
        "# -------------------------------------------------\n",
        "# 3. Install the correct ONNX Runtime build\n",
        "# -------------------------------------------------\n",
        "import torch, subprocess, sys, pathlib, shutil   # import utilities weâ€™ll need later\n",
        "\n",
        "def pip_install(pkgs):                              # tiny helper that runs pip install in a subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
        "\n",
        "if torch.cuda.is_available():                       # if a GPU is present, install the GPUâ€‘enabled runtime\n",
        "    pip_install([\"onnxruntime-gpu\"])\n",
        "else:                                               # otherwise fall back to the CPUâ€‘only runtime\n",
        "    pip_install([\"onnxruntime\"])\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. Download Whisperâ€‘tiny ONNX repo\n",
        "# -------------------------------------------------\n",
        "from huggingface_hub import snapshot_download      # utility to download a repo (or part of it) from HF Hub\n",
        "\n",
        "onnx_repo_id = \"onnx-community/whisper-large-v3-turbo-ONNX\"      # repo that contains the ONNXâ€‘exported Whisperâ€‘tiny\n",
        "onnx_dir = snapshot_download(                      # download only the files we actually need\n",
        "    repo_id=onnx_repo_id,\n",
        "    allow_patterns=[                               # keep only the listed patterns\n",
        "        \"*.onnx\",                                 # all ONNX model files\n",
        "        \"config.json\",\n",
        "        \"*_data\",                                 # external weight files (e.g. encoder_model.onnx_data)\n",
        "        \"generation_config.json\",                 # model config (used by WhisperProcessor)\n",
        "        \"preprocessor_config.json\",               # featureâ€‘extractor config\n",
        "        \"tokenizer_config.json\",                  # tokenizer config\n",
        "        \"vocab.json\", \"merges.txt\",              # BPE vocab files\n",
        "        \"added_tokens.json\", \"special_tokens_map.json\",\n",
        "        \"normalizer.json\", \"tokenizer.json\",      # full tokenizer definition\n",
        "    ],\n",
        "    local_dir=\"./whisper-large-v3-turbo-ONNX\",               # where to store the files locally\n",
        "    cache_dir=\"./hf_cache\",                        # shared cache folder (speeds up reâ€‘runs)\n",
        ")\n",
        "print(\"âœ… ONNX model downloaded to:\", onnx_dir)    # sanityâ€‘check output\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. Load processor & ONNX model\n",
        "# -------------------------------------------------\n",
        "from transformers import WhisperProcessor, GenerationConfig, pipeline   # core HF classes\n",
        "from optimum.onnxruntime import ORTModelForSpeechSeq2Seq                # ONNXâ€‘accelerated Whisper model\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(onnx_dir)   # loads tokenizer + feature extractor from the ONNX folder\n",
        "\n",
        "execution_provider = (                                 # tell ONNX Runtime where to run the model\n",
        "    \"CUDAExecutionProvider\" if torch.cuda.is_available() else \"CPUExecutionProvider\"\n",
        ")\n",
        "\n",
        "ort_model = ORTModelForSpeechSeq2Seq.from_pretrained(  # load the ONNX weights as an HFâ€‘compatible model\n",
        "    onnx_dir,\n",
        "    provider=execution_provider,\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6. Build a Whisperâ€‘aware GenerationConfig (now works because the JSON exists)\n",
        "# -------------------------------------------------\n",
        "gen_cfg = GenerationConfig.from_pretrained(            # reads generation_config.json we just copied\n",
        "    onnx_dir,                                          # folder containing the JSON\n",
        "    max_new_tokens=256,                                # limit the length of the generated transcript\n",
        "    do_sample=False,                                   # Whisper is deterministic â†’ no sampling\n",
        "    language=\"de\",                                     # default language (German); can be overridden later\n",
        "    task=\"transcribe\",                                 # default task (transcribe vs. translate)\n",
        ")\n",
        "\n",
        "# (Optional sanityâ€‘check â€“ the attribute that caused the earlier crash must exist)\n",
        "assert hasattr(gen_cfg, \"lang_to_id\"), \"lang_to_id missing â€“ something went wrong!\"   # Whisper needs this mapping\n",
        "\n",
        "ort_model.generation_config = gen_cfg   # attach the config to the ONNX model instance\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 7. Build the ASR pipeline\n",
        "# -------------------------------------------------\n",
        "asr_pipe = pipeline(                                 # highâ€‘level HF pipeline for automaticâ€‘speechâ€‘recognition\n",
        "    task=\"automatic-speech-recognition\",\n",
        "    model=ort_model,                                 # the ONNX Whisper model\n",
        "    tokenizer=processor.tokenizer,                    # tokenizer from the WhisperProcessor\n",
        "    feature_extractor=processor.feature_extractor,    # feature extractor (logâ€‘mel spectrogram)\n",
        "    device=0 if torch.cuda.is_available() else -1,   # 0 â†’ first GPU, -1 â†’ CPU\n",
        "    # ignore_warning=True   # uncomment to silence the experimental chunk_length_s warning\n",
        ")\n",
        "\n",
        "print(\"âœ… Pipeline ready â€“ max_new_tokens:\",\n",
        "      asr_pipe.generation_config.max_new_tokens)   # confirm that the pipeline sees our config\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 8. Helper to load audio (wav, mp3, m4a, â€¦)\n",
        "# -------------------------------------------------\n",
        "import librosa, numpy as np, soundfile as sf, io, requests   # audio I/O & resampling libs\n",
        "import textwrap                                   # Standardâ€‘library import for handling long strings\n",
        "\n",
        "def load_audio(path_or_url, target_sr=16000):\n",
        "    \"\"\"Load a local file or a remote URL, convert to mono, and resample to 16â€¯kHz.\"\"\"\n",
        "    if isinstance(path_or_url, str) and path_or_url.startswith(\"http\"):   # remote URL?\n",
        "        resp = requests.get(path_or_url)          # download the file\n",
        "        resp.raise_for_status()                   # raise if HTTP error\n",
        "        data, sr = sf.read(io.BytesIO(resp.content))   # read with soundfile from memory\n",
        "    else:                                          # local path\n",
        "        data, sr = sf.read(path_or_url)            # read wav/mp3/etc.\n",
        "\n",
        "    # make mono â€“ Whisper expects a single channel\n",
        "    if data.ndim > 1:\n",
        "        data = data.mean(axis=1)                  # average channels\n",
        "\n",
        "    # resample to the 16â€¯kHz rate Whisper was trained on\n",
        "    if sr != target_sr:\n",
        "        data = librosa.resample(data, orig_sr=sr, target_sr=target_sr)\n",
        "\n",
        "    return data.astype(np.float32)                # ensure float32 (required by the model)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 9. Run inference\n",
        "# -------------------------------------------------\n",
        "# Upload a file to /content/audio.wav or give a public URL\n",
        "audio_path = \"/content/audio.wav\"          # <-- replace with your own file (or a URL)\n",
        "audio_array = load_audio(audio_path)       # load & preprocess the audio\n",
        "\n",
        "audio_input = {\"array\": audio_array, \"sampling_rate\": 16000}   # format expected by the pipeline\n",
        "\n",
        "# NOTE: temperature, top_p, repetition_penalty are ignored for Whisper.\n",
        "result = asr_pipe(\n",
        "    audio_input,\n",
        "    chunk_length_s=30,               # split long audio into 30â€‘second chunks (optional)\n",
        "    return_timestamps=False,         # we only want the plain transcript\n",
        "    generate_kwargs=dict(\n",
        "        task=\"transcribe\",           # Whisper task â€“ can also be \"translate\"\n",
        "        language=\"de\",               # language code (e.g. \"en\", \"de\", \"fr\", â€¦)\n",
        "        # temperature=0.6,          # ignored by Whisper â€“ left here for illustration\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 10. Prettyâ€‘print the transcription\n",
        "# -------------------------------------------------\n",
        "raw_text = result[\"text\"]                     # <-- the oneâ€‘line string\n",
        "wrapped   = textwrap.fill(raw_text, width=80) # 80â€‘char line length (adjust as you like)\n",
        "\n",
        "print(\"\\n=== Transcription ===\")\n",
        "print(wrapped)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ex4ecrkt-j8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}